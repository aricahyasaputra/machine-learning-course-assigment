{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tugas_Minggu_2_Linear_Model_ScikitLearn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYKAVGlcDWXgliHSfDWspx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aricahyasaputra/machine-learning-course-assigment/blob/main/Minggu%202%20Linear%20Model/Tugas_Minggu_2_Linear_Model_ScikitLearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Ari Cahya Saputra\n",
        "\n",
        "NIM : 1103190093\n",
        "\n",
        "Kelas : TK-42-PIL"
      ],
      "metadata": {
        "id": "R1KqOO49bQL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Models"
      ],
      "metadata": {
        "id": "XwleCYjLbWu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ordinary Least Squares"
      ],
      "metadata": {
        "id": "7iOntE0BbcNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LinearRegression` fits a linear model with coefficients\n",
        "to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation."
      ],
      "metadata": {
        "id": "hrtITMGAbim2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LinearRegression` will take in its fit method arrays X, y and will store the coefficients of the linear model in its coef_ member:"
      ],
      "metadata": {
        "id": "SAg0VxRbbp63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eVL2sIXX0Wh",
        "outputId": "f0e2b6f8-1a94-47a4-af51-5a17aa127626"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofKdDVQYb221",
        "outputId": "87b94130-f3b7-42d8-95ea-859b24abc1b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5, 0.5])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ridge regression and classification"
      ],
      "metadata": {
        "id": "t9P5c2UbdZiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Ridge` regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares:"
      ],
      "metadata": {
        "id": "WLgjIw-6dgi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.Ridge(alpha=.5)\n",
        "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])"
      ],
      "metadata": {
        "id": "V5BNBZPIdcHi",
        "outputId": "5bc5fefc-8eb4-4160-9031-ca6182519b74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ridge(alpha=0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_\n"
      ],
      "metadata": {
        "id": "zFfHl9p5dqc3",
        "outputId": "fd24c759-593c-4381-c767-4523762c8a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.34545455, 0.34545455])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg.intercept_"
      ],
      "metadata": {
        "id": "obZvUACddr8S",
        "outputId": "981e220a-70dc-40e5-dadf-39f18a4f6207",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13636363636363638"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the regularization parameter: leave-one-out Cross-Validation"
      ],
      "metadata": {
        "id": "mecEV_jfd5ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
        "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])"
      ],
      "metadata": {
        "id": "N7frnGTtd6mW",
        "outputId": "a0853e14-40df-4a16-f336-37d55dfada0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
              "       1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg.alpha_"
      ],
      "metadata": {
        "id": "WKjX6Ii1d_SQ",
        "outputId": "da4bb933-51ba-4faa-9083-68500525fcee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lasso"
      ],
      "metadata": {
        "id": "486F0uWTeGRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.Lasso(alpha=0.1)\n",
        "reg.fit([[0, 0], [1, 1]], [0, 1])"
      ],
      "metadata": {
        "id": "H_o5jQTHeF3f",
        "outputId": "10cbea83-815f-40a0-ae40-0950542d38dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lasso(alpha=0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg.predict([[1, 1]])"
      ],
      "metadata": {
        "id": "F-mLBBQ_eLbu",
        "outputId": "9afbbea9-f954-4a39-ca1b-3a5645b166ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-task Lasso"
      ],
      "metadata": {
        "id": "7HuRDIaVeZdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MultiTaskLasso is a linear model that estimates sparse coefficients for multiple regression problems jointly: y is a 2D array, of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks."
      ],
      "metadata": {
        "id": "kCPrrGogebmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Elastic-Net"
      ],
      "metadata": {
        "id": "6oMUdg88eefZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ElasticNet is a linear regression model trained with both\n",
        "and\n",
        "-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of\n",
        "and\n",
        "using the l1_ratio parameter."
      ],
      "metadata": {
        "id": "NTg6pAGCeovP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-task Elastic-Net"
      ],
      "metadata": {
        "id": "a1vk7l5uer-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MultiTaskElasticNet is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: Y is a 2D array of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks."
      ],
      "metadata": {
        "id": "L-N3vXWGeuia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Least Angle Regression"
      ],
      "metadata": {
        "id": "CGy3SUHXevL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features."
      ],
      "metadata": {
        "id": "Rwi_VnRaeyPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LARS Lasso"
      ],
      "metadata": {
        "id": "qOyn4jhHe01r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LassoLars` is a lasso model implemented using the LARS algorithm, and unlike the implementation based on coordinate descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients."
      ],
      "metadata": {
        "id": "X1--ZWose2WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.LassoLars(alpha=.1, normalize=False)\n",
        "reg.fit([[0, 0], [1, 1]], [0, 1])"
      ],
      "metadata": {
        "id": "HI5G9wlOeWM8",
        "outputId": "9e0c2567-6aba-409c-e5af-ca150db285d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LassoLars(alpha=0.1, normalize=False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_"
      ],
      "metadata": {
        "id": "F7SrLmwbe_On",
        "outputId": "d6642010-9500-483e-bf9c-f8d2f93c8846",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.6, 0. ])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Orthogonal Matching Pursuit (OMP)"
      ],
      "metadata": {
        "id": "USoN207MfHuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OrthogonalMatchingPursuit and orthogonal_mp implements the OMP algorithm for approximating the fit of a linear model with constraints imposed on the number of non-zero coefficients (ie. the\n",
        "pseudo-norm)."
      ],
      "metadata": {
        "id": "pKNCzs2vfKzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bayesian Regression"
      ],
      "metadata": {
        "id": "GlJVN_87fLdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand."
      ],
      "metadata": {
        "id": "riIMIrBjfO-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian Ridge Regression is used for regression:"
      ],
      "metadata": {
        "id": "an_G2bFMfUfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n",
        "Y = [0., 1., 2., 3.]\n",
        "reg = linear_model.BayesianRidge()\n",
        "reg.fit(X, Y)"
      ],
      "metadata": {
        "id": "Jt1KcJp9fWJ1",
        "outputId": "5370645e-6b1e-4940-e4a1-e30f7766a9c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BayesianRidge()"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After being fitted, the model can then be used to predict new values:"
      ],
      "metadata": {
        "id": "j_EMV6k_fbHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg.predict([[1, 0.]])"
      ],
      "metadata": {
        "id": "kHBVEH8kfcEb",
        "outputId": "24e6af22-4d00-4346-acca-169c18033425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.50000013])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficients of the model can be accessed:"
      ],
      "metadata": {
        "id": "APj-pLTQff15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_"
      ],
      "metadata": {
        "id": "qwiFqoIIfhhn",
        "outputId": "7103fea1-8de1-471b-f750-a1a8c1d2d660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.49999993, 0.49999993])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic regression"
      ],
      "metadata": {
        "id": "hby8D-jmfoT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function."
      ],
      "metadata": {
        "id": "D221dOc0fsSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generalized Linear Regression"
      ],
      "metadata": {
        "id": "ae1iCZWJfv3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`TweedieRegressor` implements a generalized linear model for the Tweedie distribution, that allows to model any of the above mentioned distributions using the appropriate `power` parameter. In particular:"
      ],
      "metadata": {
        "id": "tbeg8f0Mf5Nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   `power` = 0: `Normal` distribution. Specific estimators such as Ridge, ElasticNet\n",
        "are generally more appropriate in this case.\n",
        "*   `power` = 1: Poisson distribution. `PoissonRegressor` is exposed for convenience. However, it is strictly equivalent to `TweedieRegressor(power=1, link='log')`.\n",
        "\n",
        "*   `powe`r = 2: `Gamma` distribution. `GammaRegressor` is exposed for convenience. However, it is strictly equivalent to `TweedieRegressor(power=2, link='log')`.\n",
        "*   `Power` = 3: Inverse Gaussian distribution.\n",
        "\n",
        "The link function is determined by the link parameter."
      ],
      "metadata": {
        "id": "69wiBikbgCrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import TweedieRegressor\n",
        "reg = TweedieRegressor(power=1, alpha=0.5, link='log')\n",
        "reg.fit([[0, 0], [0, 1], [2, 2]], [0, 1, 2])"
      ],
      "metadata": {
        "id": "s_NuIQU7g2wG",
        "outputId": "0832830f-093f-4568-b45b-ad39e369c187",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TweedieRegressor(alpha=0.5, link='log', power=1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_"
      ],
      "metadata": {
        "id": "HhnA0ByLg3vP",
        "outputId": "65095820-0e93-495f-d6ed-6af8dd373212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.24631611, 0.43370317])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg.intercept_"
      ],
      "metadata": {
        "id": "aWi3_2N6g7_t",
        "outputId": "c7ce1473-d565-4c72-d7be-4e83dc1f680a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.7638091359123445"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stochastic Gradient Descent - SGD"
      ],
      "metadata": {
        "id": "kn98sd5chCT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The partial_fit method allows online/out-of-core learning."
      ],
      "metadata": {
        "id": "4c2Bz5jEhFlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perceptron"
      ],
      "metadata": {
        "id": "vTBO0kkFhHkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Perceptron is another simple classification algorithm suitable for large scale learning. By default:\n",
        "\n",
        "        \n",
        "*   It does not require a learning rate.\n",
        "\n",
        "*   It is not regularized (penalized).\n",
        "\n",
        "*   It updates its model only on mistakes.\n",
        "\n",
        "The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser."
      ],
      "metadata": {
        "id": "M6VVWE9DhKa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Passive Aggressive Algorithms"
      ],
      "metadata": {
        "id": "_dFd8v2ehVqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C."
      ],
      "metadata": {
        "id": "qIQcsij1hYu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Robustness regression: outliers and modeling errors"
      ],
      "metadata": {
        "id": "g-oiG1qXhbws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Robust regression aims to fit a regression model in the presence of corrupt data: either outliers, or error in the model."
      ],
      "metadata": {
        "id": "Uw_8JwVRhewJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Quantile Regression"
      ],
      "metadata": {
        "id": "tTqQcNkXhjbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantile regression may be useful if one is interested in predicting an interval instead of point prediction. Sometimes, prediction intervals are calculated based on the assumption that prediction error is distributed normally with zero mean and constant variance. Quantile regression provides sensible prediction intervals even for errors with non-constant (but predictable) variance or non-normal distribution."
      ],
      "metadata": {
        "id": "EV09r8OahnDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Polynomial regression: extending linear models with basis functions"
      ],
      "metadata": {
        "id": "Frf0ocqBhpD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n",
        "\n",
        "For example, a simple linear regression can be extended by constructing **polynomial** features from the coefficients. "
      ],
      "metadata": {
        "id": "EofcbYB4htJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This figure is created using the `PolynomialFeatures` transformer, which transforms an input data matrix into a new data matrix of a given degree. It can be used as follows:"
      ],
      "metadata": {
        "id": "QYlU-uUIhxiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "X = np.arange(6).reshape(3, 2)\n",
        "X"
      ],
      "metadata": {
        "id": "iHkO_o5Rh0-4",
        "outputId": "a7da5f06-dccb-469f-e59d-5c9ea090233d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [2, 3],\n",
              "       [4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poly = PolynomialFeatures(degree=2)\n",
        "poly.fit_transform(X)"
      ],
      "metadata": {
        "id": "MamuQQHeh12u",
        "outputId": "3b48b91f-0335-49cd-e90e-1a50d577e63d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
              "       [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
              "       [ 1.,  4.,  5., 16., 20., 25.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sort of preprocessing can be streamlined with the Pipeline tools. A single object representing a simple polynomial regression can be created and used as follows:"
      ],
      "metadata": {
        "id": "gHLBjYciiAm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "model = Pipeline([('poly', PolynomialFeatures(degree=3)),\n",
        "                  ('linear', LinearRegression(fit_intercept=False))])\n",
        "# fit to an order-3 polynomial data\n",
        "x = np.arange(5)\n",
        "y = 3 - 2 * x + x ** 2 - x ** 3\n",
        "model = model.fit(x[:, np.newaxis], y)\n",
        "model.named_steps['linear'].coef_"
      ],
      "metadata": {
        "id": "cjix0Iy4iDmA",
        "outputId": "fc2a8596-2fd8-488c-bf5f-7a6b6343285d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3., -2.,  1., -1.])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can solve the XOR problem with a linear classifier:"
      ],
      "metadata": {
        "id": "Z0v48Ps4iRS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = X[:, 0] ^ X[:, 1]\n",
        "y"
      ],
      "metadata": {
        "id": "vTTa9s9WiEWe",
        "outputId": "fadf047d-fdfb-4b16-ddf2-f5d287602260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)\n",
        "X"
      ],
      "metadata": {
        "id": "xzApITB1iU98",
        "outputId": "875eb52a-557d-45ec-91bd-30ee0a2f78f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0],\n",
              "       [1, 0, 1, 0],\n",
              "       [1, 1, 0, 0],\n",
              "       [1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,\n",
        "                 shuffle=False).fit(X, y)"
      ],
      "metadata": {
        "id": "Iw6Io6LlicRr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the classifier “predictions” are perfect:"
      ],
      "metadata": {
        "id": "7Cl7eINFii3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict(X)"
      ],
      "metadata": {
        "id": "NzWM3G0Zih7Q",
        "outputId": "4302dd21-c686-442d-fc6a-5d96ea27c672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X, y)"
      ],
      "metadata": {
        "id": "s6PLhFoyilif",
        "outputId": "866c65d1-d76f-4bad-a4ea-9fb092e49d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}